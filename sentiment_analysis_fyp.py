# -*- coding: utf-8 -*-
"""SENTIMENT_ANALYSIS_FYP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KZK17kTOCsl0nzid_6aOIWT4yf4byaZ3
"""

!pip install transformers datasets torch scikit-learn

from google.colab import files
uploaded = files.upload()

import pandas as pd

# Load your Roman Urdu dataset
df = pd.read_csv("Roman Urdu DataSet.csv")

# Print the column names
print("Your dataset columns are:")
print(df.columns)

import pandas as pd

df = pd.read_csv("Roman Urdu DataSet.csv")

# Display all column names properly
print("Actual column names in the dataset:")
for i, col in enumerate(df.columns):
    print(f"{i+1}. '{col}'")

# Load the CSV
df = pd.read_csv("Roman Urdu DataSet.csv")

# Show the actual column names
print("Column names:", df.columns.tolist())

# Remove extra spaces from column names
df.columns = df.columns.str.strip()

# Rename for simplicity (optional but clean)
df.rename(columns={'RomanUrduText': 'text', 'Sentiment': 'sentiment'}, inplace=True)

import pandas as pd

# Load the dataset
df = pd.read_csv("Roman Urdu DataSet.csv")

# Print column names to debug
print("Original column names:", df.columns.tolist())

# Strip spaces from column names
df.columns = df.columns.str.strip()

# Rename if needed (you can adjust based on the actual names printed above)
df.rename(columns={'RomanUrduText': 'text', 'Sentiment': 'sentiment'}, inplace=True)

# Print again to confirm
print("After cleanup:", df.columns.tolist())

# Check a few rows
print(df.head())

# Load the dataset
df = pd.read_csv("Roman Urdu DataSet.csv")

# Show column names to debug issues
print("Original Columns:", df.columns.tolist())

# Strip extra spaces and lowercase the column names
df.columns = df.columns.str.strip().str.lower()
print("Cleaned Columns:", df.columns.tolist())

# Rename columns based on cleaned names
# Example: If 'romanurdu text' and 'sentiment' are the actual names
df.rename(columns={'romanurdu text': 'text', 'sentiment': 'sentiment'}, inplace=True)

# Map sentiment to numerical labels
label_mapping = {'positive': 0, 'neutral': 1, 'negative': 2}
df['label'] = df['sentiment'].str.strip().map(label_mapping)

# Drop missing rows
df.dropna(subset=['text', 'label'], inplace=True)

# Check if everything looks correct
print(df.head())

# Import necessary libraries
import pandas as pd
import torch
from sklearn.model_selection import train_test_split
from datasets import Dataset
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# Set device (use GPU if available)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Load dataset
df = pd.read_csv("uber_reviews_without_reviewid.csv")

# Check column names
print("Dataset Columns:", df.columns)

# Convert numerical scores to sentiment labels
def map_sentiment(score):
    if score >= 4:
        return 2  # Positive
    elif score == 3:
        return 1  # Neutral
    else:
        return 0  # Negative

df['label'] = df['score'].apply(map_sentiment)

# Preprocess text (convert to lowercase & remove extra spaces)
def preprocess_text(text):
    return str(text).lower().strip()

df['cleaned_text'] = df['content'].apply(preprocess_text)

# Split dataset into training (80%) and testing (20%)
train_texts, test_texts, train_labels, test_labels = train_test_split(
    df['cleaned_text'], df['label'], test_size=0.2, random_state=42
)

# Convert into Hugging Face Dataset format
train_data = Dataset.from_dict({"text": train_texts.tolist(), "label": train_labels.tolist()})
test_data = Dataset.from_dict({"text": test_texts.tolist(), "label": test_labels.tolist()})

print(f"Training Data Size: {len(train_data)}, Testing Data Size: {len(test_data)}")

# Load BERT tokenizer and model
model_name = "bert-base-uncased"
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3).to(device)

# Tokenize text data
def tokenize_function(batch):
    return tokenizer(batch["text"], padding="max_length", truncation=True, max_length=128)

train_data = train_data.map(tokenize_function, batched=True)
test_data = test_data.map(tokenize_function, batched=True)

# Remove unnecessary columns
train_data = train_data.remove_columns(["text"])
test_data = test_data.remove_columns(["text"])

# Convert datasets to PyTorch format
train_data.set_format("torch")
test_data.set_format("torch")

# Create a Data Collator to handle batch padding
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# Define evaluation metrics
def compute_metrics(pred):
    logits, labels = pred
    predictions = torch.tensor(logits).argmax(dim=-1)  # Convert to tensor before argmax
    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average="weighted")
    acc = accuracy_score(labels, predictions)
    return {"accuracy": acc, "precision": precision, "recall": recall, "f1": f1}

# Set Training Arguments
training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=2,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=10,
    save_strategy="epoch",
    load_best_model_at_end=True,
    report_to="none"  # âœ… Disables W&B logging
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_data,
    eval_dataset=test_data,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

# Train the model
trainer.train()

# Evaluate model performance
results = trainer.evaluate()
print("Evaluation Results:", results)

# Save model and tokenizer for future use
model.save_pretrained("./fine_tuned_bert")
tokenizer.save_pretrained("./fine_tuned_bert")

print("Model saved successfully.")

from transformers import pipeline

# Load fine-tuned model
model_path = "./fine_tuned_bert"
sentiment_pipeline = pipeline("text-classification", model=model_path, tokenizer=model_path)

# Example: Predict sentiment of a university bus review
review = "The bus was late, and the driver was rude."
prediction = sentiment_pipeline(review)
print(prediction)  # Output: [{'label': 'LABEL_0', 'score': 0.98}]

# Map model output to sentiment labels
label_map = {0: "Negative", 1: "Neutral", 2: "Positive"}

# Get prediction
predicted_label = prediction[0]['label']
predicted_sentiment = label_map[int(predicted_label.split("_")[-1])]  # Convert label to readable text
print(f"Predicted Sentiment: {predicted_sentiment}")

from transformers import pipeline

# Load the fine-tuned model
model_path = "./fine_tuned_bert"  # Make sure this is the correct path
sentiment_pipeline = pipeline("text-classification", model=model_path, tokenizer=model_path)

# Example test reviews
reviews = [
    "The bus was late by 30 minutes and the driver was rude.",
    "The ride was smooth, and the driver was very polite.",
    "The bus was crowded, and the ride was just average. Nothing special."
]

# Make predictions
predictions = sentiment_pipeline(reviews)

# Display results
for review, prediction in zip(reviews, predictions):
    print(f"Review: {review}")
    print(f"Predicted Sentiment: {prediction['label']}\n")

# Mapping of model labels to sentiment categories
label_map = {0: "Negative", 1: "Neutral", 2: "Positive"}

# Convert labels and print results
for review, prediction in zip(reviews, predictions):
    label_number = int(prediction['label'].split("_")[-1])  # Extract label index
    sentiment = label_map[label_number]  # Convert to readable format

    print(f"Review: {review}")
    print(f"Predicted Sentiment: {sentiment}\n")



